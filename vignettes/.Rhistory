layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
summary(model_emnist)
model_emnist %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
history_emnist <- model_emnist %>%
fit(emnist_x_train, emnist_y_train, epochs = 5,
validation_data = list(emnist_x_valid, emnist_y_valid))
print(history_emnist)
model_emnist2 <- keras_model_sequential()
model_emnist2 %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.8) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.8) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.8) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
summary(model_emnist2)
model_emnist2 %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
history_emnist2 <- model_emnist2 %>%
fit(emnist_x_train, emnist_y_train, epochs = 5,
validation_data = list(emnist_x_valid, emnist_y_valid))
model_emnist2 <- keras_model_sequential()
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.2) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.2) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
model_emnist2 <- keras_model_sequential()
model_emnist2 %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.2) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.2) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
summary(model_emnist2)
model_emnist2 %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
history_emnist2 <- model_emnist2 %>%
fit(emnist_x_train, emnist_y_train, epochs = 5,
validation_data = list(emnist_x_valid, emnist_y_valid))
model_emnist2 <- keras_model_sequential()
model_emnist2 %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# layer_dropout(rate = 0.2) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# layer_dropout(rate = 0.2) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
# layer_dropout(rate = 0.2) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
summary(model_emnist2)
model_emnist2 %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
history_emnist2 <- model_emnist2 %>%
fit(emnist_x_train, emnist_y_train, epochs = 5,
validation_data = list(emnist_x_valid, emnist_y_valid))
getwd()
save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
library(bis557)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
load(emnist)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
load(emnist)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
ata(emnist)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
data(emnist)
library(bis557)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
data(emnist)
library(bis557)
library(ggplot2)
data(ridge_train)
data(emnist)
?data
library(bis557)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
data(emnist)
library(bis557)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
data(emnist)
# Documents/courses/BIS 557/HW_packages/bis557/vignettes
# emnist <- read_csv("./emnist-letters-train.csv")
# save(emnist, file = "/Users/chang/Documents/courses/BIS 557/HW_packages/bis557/data/emnist.rda")
data(emnist)
sample(1:10,3)
# Simulate a dataset
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
dim(X)
a = seq_along(X)
length(x)
length(a)
head(a)
a = seq_len(X)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
set.seed(1234)
X[sample(seq_along(X),50)] <- runif(50, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, eval = "mae")
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, eval = "mae")
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, eval = "mae")
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, eval = "mae")
data(emnist)
function(y, a)
{
if (y>a) d = 1
if (y<a) d = -1
if (y==a) d = 0
return(d)
}
casl_util_mae_p <-
function(y, a)
{
if (y>a) d = 1
if (y<a) d = -1
if (y==a) d = 0
return(d)
}
# Simulate a dataset
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
set.seed(1234)
X[sample(seq_along(X),50)] <- runif(50, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, eval = "mae")
library(bis557)
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
getwd()
# Simulate a dataset
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
set.seed(1234)
X[sample(seq_along(X),50)] <- runif(50, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
library(bis557)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mse")
y_pred <- casl_nn_predict(weights, X)
(y-y_pred)^2
head(y_pred)
head(weights)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
y_pred <- casl_nn_predict(weights, X)
(y-y_pred)^2
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
set.seed(1234)
X[sample(seq_along(X),5)] <- runif(5, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
y_pred <- casl_nn_predict(weights, X)
(y-y_pred)^2
head(x)
head(X)
head(y)
head(weights)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
set.seed(1234)
#X[sample(seq_along(X),5)] <- runif(5, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
y_pred <- casl_nn_predict(weights, X)
head((y-y_pred)^2)
head(X)
head(y)
head(y_pred)
head(y)
set.seed(2345)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
#X[sample(seq_along(X),5)] <- runif(5, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mae")
y_pred <- casl_nn_predict(weights, X)
head((y-y_pred)^2)
set.seed(2345)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
#X[sample(seq_along(X),5)] <- runif(5, min = 2, max = 3)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mse")
y_pred <- casl_nn_predict(weights, X)
head((y-y_pred)^2)
casl_nn_sgd <-
function(X, y, sizes, epochs, eta, weights=NULL, loss = "mse")
{
if (is.null(weights))
{
weights <- casl_nn_make_weights(sizes)
}
for (epoch in seq_len(epochs))
{
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
browser()
} }
weights }
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01, loss = "mse")
f_obj
b_obj
weights[[1]]
weights[[2]]
length(weights)
eposh
epoch
seq_len(epoch)
seq_len(epochs)
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
f_obj
}
f_obj
b_obj
weights
weights[[1]]
weights[[2]]
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
for (i in seq_len(nrow(X)))
{
f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
if (loss == "mse"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)}
if (loss == "mae"){
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mae_p)
}
for (j in seq_along(seq_along(weights)))
{
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
weights[[1]]
}
}
}
