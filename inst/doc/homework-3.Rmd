---
title: "homework-2"
author: "Chang Su"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The Vignette for Homework 3}
-->

# 1. CASL page 117, question 7.
 Kernels can also be used as density estimators. Specifically, we have
$$f_h(x)=\dfrac{1}{n}\sum_{i}K_h(x-x_i)$$
In this setting we see again why it is important to have the integral of the kernel equal to 1. Write a function kern_density that accepts a training vector x, bandwidth h, and test set x_new, returning the kernel density estimate from the Epanechnikov kernel. Visually test how this performs for some hand constructed datasets and bandwidths.

* Importance of integral:
$\int f_h(x)dx=\dfrac{1}{n}\sum_{i} \int K_h(x-x_i)dx = 1$

In other words, the property that the integral of the kernel equal to 1 is important in the sense that it enables the integral of kernel density estimate to be equal to 1, satisfying the basic requirement for a function to be a density.

Next, illustrate the function kernel_estimates by estimating the density of some test data based on the train data from the same distribution.

```{r}
# First, construct a dataset, where data is from a mixture of normal distribution
set.seed(1234)
d <- c(rnorm(500,10,1), rnorm(500,0,3))
hist(d)
# Split the data into train and test set (7:3)
train_index <- sample.int(1000,700)
dtrain <- d[train_index]
dtest <- d[!(1:1000 %in% train_index)]
```

```{r, fig.width=8, fig.height=6}
# Calculate the kernel density estimation for a bunch of h
hs <- seq(0.1,3.1,by=1)
kernel_estimates <- sapply(hs, function(h){
  kern_density(dtrain, dtest, h)
})
dtest_df <- data.frame(kern = as.vector(kernel_estimates),
                       bandwidth = rep(hs, each = length(dtest)),
                       data = dtest)
# Visualize the density estiamted by Epanechnikov kernel with 4 different bandwidths.
library(ggplot2)
ggplot(dtest_df) + geom_line(aes(x = data, y = kern, color = as.factor(bandwidth))) + scale_color_discrete(name = "Bandwidth") +
  labs(y = "Density Estimate", x = "Test Data", title = "Kernel Density Estimates for Different Choices of Bandwidths") +
  theme(legend.position="bottom", title = element_text(size=14), legend.text=element_text(size=12))
```

It's clear from the plot of 'Kernel Density Estimates for Different Choices of Bandwidths' that, 

- The estimate is 'smoother' if the bandwidth is larger. If it is too small, the estimated density would be overfitted and dragged away by single data point; while if it is too big, the estimated density would be over-smooth / flat.

- The kernel density estimation can give goood estimate of the true density when bandwidth is chosen properly. (The green line gives the best fit among 4 candidates of h, which is close the true distribution, a mixture of Gaussian)


# 2. CASL page 200, question 3
Show that if f and g are both convex functions, then their sum must also be convex.

$\textbf{Solution}$

Suppose f and g are functions on the domain X

If f is convex, then $\forall x_1,x_2 \in X, \forall \alpha \in [0,1], f(\alpha x_1 + (1-\alpha)x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2)$. Same for g if g is convex.

Then $(f+g)(\alpha x_1 + (1-\alpha)x_2) \leq \alpha (f+g)(x_1) + (1-\alpha) (f+g)(x_2)$. 

As a result, their sum is also convex.

# 3. CASL page 200, question 4
Illustrate that the absolute value function is convex. Using the result from the previous exercise, show that the l1-norm is also convex.

$\textbf{Solution}$

Let $f(x)=|x|$ denote absolute value function.

$\because |\alpha x_1 + (1-\alpha)x_2| \leq \alpha |x_1| + (1-\alpha)|x_2|, \forall x_1,x_2 \in X, \forall \alpha \in [0,1]$

The absolute value function is convex.

$\because$ l1-norm of a vector X is the sum of the absolute value function on each coordinate of X.

$\therefore$ By the previous exercise, l1-norm is also convex.

# 4. CASL page 200, question 5
Prove that the elastic net objective function is convex using the results from the previous two exercises.

$\textbf{Solution}$

The objective function of elastic net:

$$f(\beta;\lambda,\alpha)=\dfrac{1}{2n}||Y-X\beta||_2^2+\lambda((1-\alpha)\dfrac{1}{2}||\beta||_2^2+\alpha||\beta||_1)$$

$\forall x_1,x_2 \in X, \forall \alpha \in [0,1], [\alpha x_1 + (1-\alpha)x_2]^2 - [\alpha x_1^2+(1-\alpha)x_2^2]=-\alpha(1-\alpha)(x_1-x_2)^2 \leq 0$

$\therefore f(x)=x^2$ is convex.

By question 3, L2-norm is also convex.

$\because \dfrac{1}{2n}>0, \lambda(1-\alpha)\dfrac{1}{2}>0, \lambda \alpha >0$ and L1 is convex.

The object function $f(\beta;\lambda,\alpha)$ is convex.

# 5. CASL page 200, question 6
Find the KKT conditions for glmnet when 0 < α ≤ 1; write a function that checks for the KKT conditions for alpha=1 and show that it works

Recall the KKT condition to check when updating beta:

$$\dfrac{1}{n}\sum_{i=1}^n x_{il}(y_i-\sum_{j=1}^px_{ij}\hat{\beta_j})=\lambda s_l$$

where
\begin{aligned}
s_l \in  \begin{cases}
1 & \beta_j > 0 \\
-1 & \beta_j < 0 \\
[-1,1] & \beta_j = 0 \\
\end{cases}
\end{aligned}

```{r,message=FALSE}
library(glmnet)
# Generate a dataset with n<p and with only few variables actually contributing to the prediction
# This idea credit to https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html
set.seed(1234)
n <- 100
p <- 200
p_true <- 10
x <- matrix(rnorm(n*p), nrow = n, ncol = p)
y <- apply(x[,1:p_true], 1, sum) + rnorm(n)
```

$\textbf{Justify the check_kkt function}$: use it to check the kkt condition under $\lambda=0.1$ for beta coefficients obtained from $\lambda=0.11$.

Suppose we have fitted the slope coefficient under $\lambda_1$, $\beta(\lambda_1)$, where some are inactive (=0) and some are not. Now we are going to fit the LASSO for the next penalty parameter, $\lambda_2$.

Suppose $\lambda_2$ is numerically close to $\lambda_1$, it's straightforward that we will refit the model with $\beta_{\lambda_1}$ as starting point.

Here's where active set screening comes into play. Based on the intuition that the $\beta$ s inactive under $\lambda_1$ are also likely to be inactive under $\lambda_2$, active set screening tells the would-stay-inactive $\beta$ s and change-to-active $\beta$ s by checking the KKT condition for each predictor, greatly reduce the computational cost because only few active $\beta$ s would eneter the coordinate descent procedure.

Given the scenario it's used, we implement a check_kkt function and it's going to be justified in the following steps:

1. Generate a high-dimensional data set where it's proper to use LASSO

2. Fit the LASSO with $\lambda_1=0.11$

3. Apply $\text{check_kkt}$ to select the active set of $\beta_{\lambda_1=0.11}$ under $\lambda_2=0.1$.

4. Check if the would-stay-inactive $\beta$ s are 0 even after the model is refitted with $\lambda_2=0.1$. If so, we can argue that the check_kkt is correctly implemented.

```{r}
# Try two similar penalty parameters and fit LASSO with glmnet
lambdas <- c(0.1, 0.11)
LASSO_fit <- glmnet(x, y, family="gaussian", alpha=1, lambda = lambdas)

kkt_status = check_kkt(x, y, as.vector(LASSO_fit$beta[,2]), lambdas[1])
# The predictor checked to be inactive would stay inactive.
all(LASSO_fit$beta[,1][kkt_status$stay_inactive] == 0)
# Indeed, active set screening save a lot of computation cost
sum(kkt_status$stay_inactive)
```



